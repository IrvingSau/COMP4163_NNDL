{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras\n",
    "Turing QIU/Yvonne ZHANG/Ryan WANG\n",
    "\n",
    "Last modified in 2019/09/25 - 22:38\n",
    "\n",
    "![keras_logo](keras.png)\n",
    "\n",
    "## Introduction\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of *TensorFlow, CNTK, or Theano*. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "- Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "- Runs seamlessly on CPU and GPU.\n",
    "\n",
    "Keras is compatible with: Python **2.7-3.6**.\n",
    "\n",
    "\n",
    "\n",
    "### Why this name, Keras?\n",
    "Keras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\n",
    "\n",
    "Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n",
    "\n",
    "*\"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\" Homer, Odyssey 19. 562 ff (Shewring translation).*\n",
    "\n",
    "\n",
    "## Installation of Keras\n",
    "In this course, we recommend to use the TensorFlow as the backend of Keras. If you are intersted in the installation of other backends, please find the resources in the following links.\n",
    "\n",
    "- [TensorFlow installation instructions](https://www.tensorflow.org/install/#tensorflow) \n",
    "- [Theano installation instructions](http://deeplearning.net/software/theano/install.html#install) \n",
    "- [CNTK installation instructions](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-your-machine) \n",
    "\n",
    "### · Install Keras from Terminal (Mac OS):\n",
    "\n",
    "In this section, we will introduce the installation of Keras in Mac OS by terminal. First of all, we need to install the TensorFlow backend by using the pip instruction:\n",
    "\n",
    "`\n",
    "pip3 install tensorflow\n",
    "`\n",
    "\n",
    "Then, you can install Keras itself by pip:\n",
    "\n",
    "`\n",
    "pip3 install keras\n",
    "`\n",
    "\n",
    "If the installations of TensorFlow and Keras have been completed successfully, we could test it by `import` in python.\n",
    "\n",
    "![test_envs](test_envs.png)\n",
    "\n",
    "\n",
    "### · Install Keras from Anaconda (reconmmended):\n",
    "\n",
    "First, let's create a new environment in Anaconda (By following Rehan and Justices' material). \n",
    "\n",
    "![Create environment](create_envs.png)\n",
    "\n",
    "Then we need to activate the created environment. There are two ways for you to activate the environment: \n",
    "\n",
    "1) Activate the environment by the Anaconda Navigation like this:\n",
    "\n",
    "![Navigation_Activate](navig_activate.png)\n",
    "\n",
    "2) Or you want to activate the created environment by Anaconda prompt shown as followed: \n",
    "\n",
    "Step 1. List all the environments by instruction.\n",
    "\n",
    "`\n",
    "conda info --envs\n",
    "`\n",
    "\n",
    "Step 2. activate the target environment (the environment created by yourself).\n",
    "\n",
    "`\n",
    "activate xxx\n",
    "`\n",
    "\n",
    "By doing these two steps, you will get into the activated environment like followed, which is the same with the interface that activated through Anaconda Navigation.\n",
    "\n",
    "![activate environment](activate_envs.PNG)\n",
    "\n",
    "\n",
    "After the environment is activated, we could install the TensorFlow backend by using the pip instruction:\n",
    "\n",
    "`\n",
    "pip install tensorflow\n",
    "`\n",
    "\n",
    "Then, you can install Keras itself by pip:\n",
    "\n",
    "`\n",
    "pip install keras\n",
    "`\n",
    "\n",
    "Note: These installation steps assume that you are on Windows. If you are on a Linux or Mac environment, please contact Yvonne and Ziying Wang for help.\n",
    "\n",
    "### · Alternatively: install Keras from the GitHub source:\n",
    "\n",
    "First, clone Keras using `git`:\n",
    "\n",
    "```\n",
    "git clone https://github.com/keras-team/keras.git\n",
    "```\n",
    "\n",
    "Then, `cd` to the Keras folder and run the install command:\n",
    "\n",
    "```\n",
    "cd keras\n",
    "sudo python setup.py install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get Stated with Keras in MNIST\n",
    "\n",
    "Before we create our first neural network model in Keras, let us prepare our data first. As the \"Hello world\" program in deep lenarning, MNIST (Mixed National Institute of Standards and Technology database) dataset has been widely used by the students in DL and ML for their first model construction train and test. Fortunately, Keras provides a convenient module to load the MNIST dataset.\n",
    "\n",
    "### Introduction to MNIST dataset\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "\n",
    "![mnist](mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data review\n",
    "\n",
    "1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import MNIST modules，and you can load both train and test data set by load_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data records = 60000\n",
      "test data records = 10000\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()\n",
    "\n",
    "print('train data records =', len(x_train_image))\n",
    "print('test data records =', len(x_test_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display the format of image and label in train dataset: x_train_image has 60,000 pieces of data with a size of 28*28. Y_train_label also contains 60,000 records with 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train image format = (60000, 28, 28)\n",
      "y_train label format = 1\n"
     ]
    }
   ],
   "source": [
    "print('x_train image format =', x_train_image.shape)\n",
    "print('y_train label format =', len(y_train_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Define a function to show the mnist data image\n",
    "\n",
    "Define a function named plot_image() to display the image in the dataset.**Don't worry about this code :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(2, 2)\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using plot_image() to shows the first image of the mnist dataset with the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    }
   ],
   "source": [
    "plot_image(x_train_image[0])\n",
    "print('Label: ', y_train_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "After loading the data, we need to preprocess it. Our data are mainly digital image files and labels in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **For image:**\n",
    "\n",
    "These digital image files are 28 x 28 black-and-white images. We need to convert them into one-dimensional vectors, i.e. 784-length data (28x28x1 = 784, 1 represents a channel, and 3 channels for color RGB).\n",
    "\n",
    "1). Viewing **Data Dimension**: We can see that the number of images of train dataset is 60,000, and the dimension is 28*28. The number of Labels is also 60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_image: (60000, 28, 28)\n",
      "y_train_label: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train_image:', x_train_image.shape)\n",
    "print('y_train_label:', y_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **Convert image data to one dimension**: change the dimension of dataset by using reshape(), and then change data attribute to float with astype(). Finally, we can see that its dimension has become 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (60000, 784)\n",
      "x_test: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_Train = x_train_image.reshape(60000, 784).astype('float32')\n",
    "x_Test = x_test_image.reshape(10000, 784).astype('float32')\n",
    "\n",
    "print('x_train:', x_Train.shape)\n",
    "print('x_test:', x_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3). Look at **the content of Picture** 0: Each number is between 0 and 255, representing the gray scale-depth of each pixel point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198\n",
      "  198 198 170  52   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250\n",
      "  229 254 254 140   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59\n",
      "   21 236 254 106   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   83 253 209  18   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22\n",
      "  233 255  83   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129\n",
      "  254 238  44   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249\n",
      "  254  62   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254\n",
      "  187   5   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248\n",
      "   58   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_image[0])\n",
    "print(x_test_image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4). **Standardization of image data**: The process of adjusting the \"average\" of a set of data to 0 and the \"standard deviation\" to 1 is called **standardization**. The simplest way to deal with gray scale images is to divide them by 255. Finally, the value of print's No.0 data is found to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.32941177 0.7254902\n",
      " 0.62352943 0.5921569  0.23529412 0.14117648 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.87058824 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.94509804 0.7764706  0.7764706  0.7764706  0.7764706\n",
      " 0.7764706  0.7764706  0.7764706  0.7764706  0.6666667  0.20392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2627451  0.44705883 0.28235295 0.44705883 0.6392157  0.8901961\n",
      " 0.99607843 0.88235295 0.99607843 0.99607843 0.99607843 0.98039216\n",
      " 0.8980392  0.99607843 0.99607843 0.54901963 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06666667 0.25882354 0.05490196\n",
      " 0.2627451  0.2627451  0.2627451  0.23137255 0.08235294 0.9254902\n",
      " 0.99607843 0.41568628 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3254902  0.99215686 0.81960785 0.07058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.08627451\n",
      " 0.9137255  1.         0.3254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.5058824  0.99607843 0.93333334\n",
      " 0.17254902 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.23137255 0.9764706  0.99607843 0.24313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52156866 0.99607843\n",
      " 0.73333335 0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.8039216  0.972549   0.22745098 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.49411765\n",
      " 0.99607843 0.7137255  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.29411766 0.9843137  0.9411765  0.22352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.07450981\n",
      " 0.8666667  0.99607843 0.6509804  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.79607844 0.99607843 0.85882354\n",
      " 0.13725491 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14901961 0.99607843 0.99607843 0.3019608  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12156863 0.8784314  0.99607843\n",
      " 0.4509804  0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.52156866 0.99607843 0.99607843 0.20392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23921569 0.9490196\n",
      " 0.99607843 0.99607843 0.20392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.4745098  0.99607843 0.99607843 0.85882354\n",
      " 0.15686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4745098  0.99607843 0.8117647  0.07058824 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "x_Train_normalize = x_Train/255\n",
    "x_Test_normalize = x_Test/255\n",
    "\n",
    "print(x_Train_normalize[0])\n",
    "print(x_Test_normalize[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **For labels:**\n",
    "\n",
    "Label's data content ranges from 0 to 9, so we can convert it into 10 combinations of 0 or 1, for example, 7 to 0000000100 and 5 to 00000010000. This method is called \"One-hot encoding\", and is mainly applied to the conversion of classification values.\n",
    "\n",
    "1. Let's first look at the first five label data in train dataset and find out that the values are 5, 0, 4, 1, 9 and the type is uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we use **np_utils.to_categorical()** to perform one-hot encoding transformation. The first five pieces of data from print show that 5 becomes 00000010000, 0 becomes 1000000000, 4 becomes 0000100000...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_TrainOneHot = np_utils.to_categorical(y_train_label)\n",
    "y_TestOneHot = np_utils.to_categorical(y_test_label)\n",
    "\n",
    "y_TrainOneHot[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and accuracy history visulization\n",
    "\n",
    "Next, let us write a LossHistory class for the visulization of the Loss and Accuracy, **just for fun, no need to worry :)**, you can skip this section rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get start with Model\n",
    "\n",
    "The core data structure of Keras is a **model**, a way to organize layers. The simplest type of model is the `Sequential` model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking layers is as easy as `.add()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "# Stack the layers into model\n",
    "# 1st layer: Dense layer\n",
    "model.add(Dense(500,input_shape=(784,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.8))  # 50% dropout\n",
    "# 2nd layer: Dense layer\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.8)) # 50% dropout\n",
    "# 3rd layer: Dense layer\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.8)) # 50% dropout\n",
    "# Output layer: Dense layer with Dimension in 10 (one dimension for one category)\n",
    "# Softmax: Widely used in classification problem, output the probability of each category\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model looks good, configure its learning process with `.compile()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function, optimizer (Method used in training, 'sgd' denotes stochastic gradient descent) and record the accuracy for each traning epoch\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the model compile (For example, you want to customize the lr (aka learning rate))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create a history instance of our little calss to record the callbacks information during training process\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now iterate on your training data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 1.3601 - acc: 0.5627 - val_loss: 0.4752 - val_acc: 0.8681\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.7665 - acc: 0.7499 - val_loss: 0.4038 - val_acc: 0.8904\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.6652 - acc: 0.7891 - val_loss: 0.3721 - val_acc: 0.8994\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.6150 - acc: 0.8112 - val_loss: 0.3529 - val_acc: 0.9064\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.5768 - acc: 0.8234 - val_loss: 0.3460 - val_acc: 0.9086\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.5462 - acc: 0.8340 - val_loss: 0.3347 - val_acc: 0.9114\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.5287 - acc: 0.8421 - val_loss: 0.3255 - val_acc: 0.9120\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.5126 - acc: 0.8471 - val_loss: 0.3199 - val_acc: 0.9168\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.4961 - acc: 0.8524 - val_loss: 0.3121 - val_acc: 0.9180\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4861 - acc: 0.8579 - val_loss: 0.3082 - val_acc: 0.9168\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.4783 - acc: 0.8598 - val_loss: 0.3022 - val_acc: 0.9185\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.4720 - acc: 0.8609 - val_loss: 0.2957 - val_acc: 0.9214\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.4619 - acc: 0.8648 - val_loss: 0.2993 - val_acc: 0.9204\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4514 - acc: 0.8673 - val_loss: 0.3032 - val_acc: 0.9212\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4447 - acc: 0.8699 - val_loss: 0.2921 - val_acc: 0.9224\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4351 - acc: 0.8739 - val_loss: 0.2922 - val_acc: 0.9245\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.4363 - acc: 0.8738 - val_loss: 0.2887 - val_acc: 0.9238\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4304 - acc: 0.8758 - val_loss: 0.2863 - val_acc: 0.9257\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.4170 - acc: 0.8789 - val_loss: 0.2866 - val_acc: 0.9261\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.4223 - acc: 0.8790 - val_loss: 0.2873 - val_acc: 0.9262\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.4122 - acc: 0.8809 - val_loss: 0.2821 - val_acc: 0.9265\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4140 - acc: 0.8803 - val_loss: 0.2721 - val_acc: 0.9300\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.4002 - acc: 0.8863 - val_loss: 0.2832 - val_acc: 0.9271\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.4049 - acc: 0.8843 - val_loss: 0.2729 - val_acc: 0.9287\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4026 - acc: 0.8830 - val_loss: 0.2720 - val_acc: 0.9288\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.3964 - acc: 0.8850 - val_loss: 0.2660 - val_acc: 0.9297\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.3927 - acc: 0.8871 - val_loss: 0.2737 - val_acc: 0.9288\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3848 - acc: 0.8890 - val_loss: 0.2623 - val_acc: 0.9317\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.3894 - acc: 0.8888 - val_loss: 0.2655 - val_acc: 0.9304\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3831 - acc: 0.8890 - val_loss: 0.2582 - val_acc: 0.9320\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3849 - acc: 0.8886 - val_loss: 0.2523 - val_acc: 0.9333\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.3760 - acc: 0.8929 - val_loss: 0.2504 - val_acc: 0.9333\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.3745 - acc: 0.8917 - val_loss: 0.2593 - val_acc: 0.9333\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.3715 - acc: 0.8931 - val_loss: 0.2544 - val_acc: 0.9332\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.3700 - acc: 0.8939 - val_loss: 0.2525 - val_acc: 0.9342\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.3639 - acc: 0.8964 - val_loss: 0.2502 - val_acc: 0.9358\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.3661 - acc: 0.8956 - val_loss: 0.2455 - val_acc: 0.9350\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.3624 - acc: 0.8967 - val_loss: 0.2462 - val_acc: 0.9347\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.3596 - acc: 0.8978 - val_loss: 0.2497 - val_acc: 0.9362\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.3619 - acc: 0.8966 - val_loss: 0.2494 - val_acc: 0.9357\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.3581 - acc: 0.8973 - val_loss: 0.2408 - val_acc: 0.9366\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.3538 - acc: 0.8993 - val_loss: 0.2412 - val_acc: 0.9376\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.3559 - acc: 0.8989 - val_loss: 0.2382 - val_acc: 0.9378\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.3526 - acc: 0.8997 - val_loss: 0.2397 - val_acc: 0.9377\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.3501 - acc: 0.9002 - val_loss: 0.2323 - val_acc: 0.9390\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3451 - acc: 0.9020 - val_loss: 0.2425 - val_acc: 0.9368\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3450 - acc: 0.9027 - val_loss: 0.2308 - val_acc: 0.9382\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.3419 - acc: 0.9022 - val_loss: 0.2373 - val_acc: 0.9371\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.3419 - acc: 0.9034 - val_loss: 0.2299 - val_acc: 0.9384\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.3386 - acc: 0.9031 - val_loss: 0.2333 - val_acc: 0.9388\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3370 - acc: 0.9047 - val_loss: 0.2312 - val_acc: 0.9390\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3403 - acc: 0.9035 - val_loss: 0.2302 - val_acc: 0.9397\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.3352 - acc: 0.9039 - val_loss: 0.2279 - val_acc: 0.9389\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3324 - acc: 0.9056 - val_loss: 0.2258 - val_acc: 0.9402\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.3331 - acc: 0.9062 - val_loss: 0.2254 - val_acc: 0.9404\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3311 - acc: 0.9055 - val_loss: 0.2195 - val_acc: 0.9419\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.3265 - acc: 0.9072 - val_loss: 0.2297 - val_acc: 0.9403\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.3291 - acc: 0.9053 - val_loss: 0.2190 - val_acc: 0.9424\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3293 - acc: 0.9073 - val_loss: 0.2201 - val_acc: 0.9414\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3250 - acc: 0.9078 - val_loss: 0.2247 - val_acc: 0.9412\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.3217 - acc: 0.9086 - val_loss: 0.2188 - val_acc: 0.9423\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.3204 - acc: 0.9106 - val_loss: 0.2181 - val_acc: 0.9425\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.3199 - acc: 0.9092 - val_loss: 0.2184 - val_acc: 0.9422\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3192 - acc: 0.9096 - val_loss: 0.2168 - val_acc: 0.9438\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.3179 - acc: 0.9109 - val_loss: 0.2168 - val_acc: 0.9439\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.3166 - acc: 0.9103 - val_loss: 0.2145 - val_acc: 0.9436\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.3149 - acc: 0.9095 - val_loss: 0.2182 - val_acc: 0.9436\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3151 - acc: 0.9109 - val_loss: 0.2098 - val_acc: 0.9437\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3143 - acc: 0.9118 - val_loss: 0.2137 - val_acc: 0.9447\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.3127 - acc: 0.9123 - val_loss: 0.2108 - val_acc: 0.9448\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.3112 - acc: 0.9122 - val_loss: 0.2109 - val_acc: 0.9444\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.3098 - acc: 0.9123 - val_loss: 0.2095 - val_acc: 0.9446\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.3073 - acc: 0.9133 - val_loss: 0.2113 - val_acc: 0.9436\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3118 - acc: 0.9118 - val_loss: 0.2121 - val_acc: 0.9441\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.3104 - acc: 0.9126 - val_loss: 0.2140 - val_acc: 0.9452\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.3087 - acc: 0.9138 - val_loss: 0.2095 - val_acc: 0.9443\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.3042 - acc: 0.9143 - val_loss: 0.2047 - val_acc: 0.9461\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.3065 - acc: 0.9129 - val_loss: 0.2076 - val_acc: 0.9454\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.3065 - acc: 0.9136 - val_loss: 0.2015 - val_acc: 0.9464\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.3012 - acc: 0.9151 - val_loss: 0.2033 - val_acc: 0.9460\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.3044 - acc: 0.9150 - val_loss: 0.2020 - val_acc: 0.9471\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.2977 - acc: 0.9162 - val_loss: 0.2038 - val_acc: 0.9463\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.2987 - acc: 0.9163 - val_loss: 0.2030 - val_acc: 0.9476\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.3022 - acc: 0.9146 - val_loss: 0.2041 - val_acc: 0.9471\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.2980 - acc: 0.9155 - val_loss: 0.2004 - val_acc: 0.9472\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2981 - acc: 0.9164 - val_loss: 0.2013 - val_acc: 0.9474\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2932 - acc: 0.9162 - val_loss: 0.2008 - val_acc: 0.9467\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.2954 - acc: 0.9171 - val_loss: 0.1971 - val_acc: 0.9466\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.2944 - acc: 0.9163 - val_loss: 0.2004 - val_acc: 0.9478\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.2948 - acc: 0.9172 - val_loss: 0.1977 - val_acc: 0.9486\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.2908 - acc: 0.9190 - val_loss: 0.1963 - val_acc: 0.9484\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.2900 - acc: 0.9186 - val_loss: 0.1997 - val_acc: 0.9478\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2886 - acc: 0.9193 - val_loss: 0.1983 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.2916 - acc: 0.9196 - val_loss: 0.1963 - val_acc: 0.9485\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.2894 - acc: 0.9188 - val_loss: 0.1937 - val_acc: 0.9488\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.2896 - acc: 0.9187 - val_loss: 0.1899 - val_acc: 0.9488\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.2864 - acc: 0.9189 - val_loss: 0.1896 - val_acc: 0.9499\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.2871 - acc: 0.9201 - val_loss: 0.1921 - val_acc: 0.9496\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.2870 - acc: 0.9192 - val_loss: 0.1973 - val_acc: 0.9496\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.2845 - acc: 0.9200 - val_loss: 0.1897 - val_acc: 0.9496\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
    "model.fit(x_Train_normalize, y_TrainOneHot, epochs=100, verbose=1, batch_size=200, shuffle=True, validation_data=(x_Test_normalize, y_TestOneHot), callbacks=[history])\n",
    "\n",
    "# Use the small funny class written by us before to plot the learning curve\n",
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train the model with 100 epochs, the learning curve should be something like the following figure, and the accuracy in testing set should be 94.96% approximately.\n",
    "\n",
    "\n",
    "![100epoch](100epoch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's back to the model, subsequently, you can start the testing step and evaluate your model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step\n",
      "The accuracy of the model in testing set is 0.949600\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_Test_normalize,y_TestOneHot,batch_size=200,verbose=1)\n",
    "\n",
    "# scores variable records two indicators of our model: scores[0] - loss; scores[1] - accuracy\n",
    "print(\"The accuracy of the model in testing set is %f\" % (scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end\n",
    "\n",
    "Comparing with TensorFlow and other deep learning framework in python, the idea of Keras is that, building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n",
    "\n",
    "In the end, let's have a glance on the difference on TensorFlow and Keras, and you will know how nice Keras are.\n",
    "\n",
    "### Code in Adding a Layer\n",
    "\n",
    "![tf_vs_keras](tf_vs_keras.png)\n",
    "\n",
    "However, as a high-level encapusulated package, Keras usually could not provide a promising customization for your network architecture, which means you need a more flexible framework to do so. For example, tensorflow and pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] https://keras.io/\n",
    "\n",
    "[2] https://medium.com/@afozbek_/how-to-train-a-model-with-mnist-dataset-d79f8123ba84\n",
    "\n",
    "[3] https://blog.csdn.net/wwxy1995/article/details/79852905\n",
    "\n",
    "[4] https://blog.csdn.net/u013381011/article/details/78911848\n",
    "\n",
    "[5] https://qffc.org\n",
    "\n",
    "[6] https://www.researchgate.net/publication/335542449_Hybrid_Chaotic_Radial_Basis_Function_Neural_Oscillatory_Network_HCRBFNON_for_Financial_Forecast_and_Trading_System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledge\n",
    "\n",
    "The authors wants to thank for the help from the UIC Artificial Intelligence Special Interest Group (UICAISIG) in the environment configuration problem, and Prof. Yann LeCun for his magnificant work in the MNIST dataset. Specially, in the yesterday, Mr. Zheng Hao companied with us to finish this material, we also want to thank to him.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UICAI_Group_Logo.png](UICAI_Group_Logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
